{
    "summary": "The code implements AutoRT, an automated robot task system using OpenAI API and language models for visualization, task generation, and complexity ranking. The robot model executes tasks with image inputs.",
    "details": [
        {
            "comment": "The code imports necessary modules and defines a class, AutoRT, for an automated robot task system. The class takes arguments like OpenAI API key, maximum tokens, and guidance for language models. It uses VISUALIZE_OBJECT_PROMPT, GENERATE_TASKS_PROMPT, FUSED_SYSTEM_PROMPT_WITH_SOP, and FILTER_TASKS_SOP_PROMPT from the prompts module.",
            "location": "\"/media/root/Prima/works/AutoRT/docs/src/autort/main.py\":0-31",
            "content": "import os\nfrom swarms import GPT4VisionAPI, OpenAIChat\nfrom autort.prompts import (\n    VISUALIZE_OBJECT_PROMPT,\n    GENERATE_TASKS_PROMPT,\n    FUSED_SYSTEM_PROMPT_WITH_SOP,\n    FILTER_TASKS_SOP_PROMPT,\n)\nfrom dotenv import load_dotenv\nfrom typing import Callable\nload_dotenv()\nopenai_api_key = os.getenv(\"OPENAI_API_KEY\")\ngemini_api_key = os.getenv(\"GENIMI_API_KEY\")\nmax_tokens = 1000\nguidance_vllm = \"Effectively visualize the object in the scene.\"\nguidance_llm = \"Generate tasks for the robot to perform.\"\nguidance_rank = \"Rank the tasks generated based on complexity and necessity of human assistance.\"\nclass AutoRT:\n    \"\"\"\n    AutoRT class represents an automated robot task system.\n    Args:\n        openai_api_key (str): The API key for OpenAI.\n        max_tokens (int, optional): The maximum number of tokens to use for text generation. Defaults to 1000.\n        guidance_vllm (str, optional): The guidance for the first language model. Defaults to guidance_vllm.\n        guidance_llm (str, optional): The guidance for the second language model. Defaults to guidance_llm."
        },
        {
            "comment": "The code defines a class with OpenAI API key, maximum tokens for text generation, and guidance prompts for three language models. It also includes a robot model to execute tasks and system prompts for the first two language models, as well as an initialized GPT4VisionAPI object as the first language model.",
            "location": "\"/media/root/Prima/works/AutoRT/docs/src/autort/main.py\":32-45",
            "content": "        guidance_rank (str, optional): The guidance for the third language model. Defaults to guidance_rank.\n        robot_model (Callable, optional): The robot model to execute the tasks. Defaults to None.\n    Attributes:\n        openai_api_key (str): The API key for OpenAI.\n        max_tokens (int): The maximum number of tokens to use for text generation.\n        guidance_vllm (str): The guidance for the first language model.\n        guidance_llm (str): The guidance for the second language model.\n        guidance_rank (str): The guidance for the third language model.\n        robot_model (Callable): The robot model to execute the tasks.\n        system_prompt_vllm (FUSED_SYSTEM_PROMPT_WITH_SOP): The system prompt for the first language model.\n        system_prompt_llm (FUSED_SYSTEM_PROMPT_WITH_SOP): The system prompt for the second language model.\n        system_prompt_rank (FUSED_SYSTEM_PROMPT_WITH_SOP): The system prompt for the third language model.\n        vllm (GPT4VisionAPI): The first language model to visualize the object in the scene."
        },
        {
            "comment": "This code defines a class called \"AutoRT\" which is an implementation of the AutoRT system. It takes parameters such as API key, max tokens, and language models for guidance. The \"run\" method is used to execute tasks based on text and image inputs.",
            "location": "\"/media/root/Prima/works/AutoRT/docs/src/autort/main.py\":46-74",
            "content": "        llm (OpenAIChat): The second language model to generate tasks for the robot.\n        filter_llm (OpenAIChat): The third language model to rank the tasks generated by the second language model.\n    Methods:\n        run(text: str, img: str, *args, **kwargs) -> Any:\n            Runs the AutoRT system to execute tasks based on the given text and image.\n    Examples:\n        >>> from autort import AutoRT\n        >>> autort = AutoRT(openai_api_key, max_tokens=1000)\n        >>> autort.run(\"There is a bottle on the table.\", \"https://i.imgur.com/2qY9f8U.png\")\n    \"\"\"\n    def __init__(\n        self,\n        openai_api_key: str,\n        max_tokens: int = 1000,\n        guidance_vllm: str = guidance_vllm,\n        guidance_llm: str = guidance_llm,\n        guidance_rank: str = guidance_rank,\n        robot_model: Callable = None,\n        *args,\n        **kwargs\n    ):\n        self.openai_api_key = openai_api_key\n        self.max_tokens = max_tokens\n        self.guidance_vllm = guidance_vllm\n        self.guidance_llm = guidance_llm"
        },
        {
            "comment": "This code initializes various components for the AutoRT system. It sets the guidance_rank, robot_model, and defines fused prompts for visualization and tasks generation. The code then creates two LLMs (Language Models): GPT4V for object visualization, and OpenAIChat for task generation based on objects in the scene. It also sets the openai_api_key and max_tokens for both models.",
            "location": "\"/media/root/Prima/works/AutoRT/docs/src/autort/main.py\":75-106",
            "content": "        self.guidance_rank = guidance_rank\n        self.robot_model = robot_model\n        self.system_prompt_vllm = FUSED_SYSTEM_PROMPT_WITH_SOP(\n            guidance=guidance_vllm,\n            sop=VISUALIZE_OBJECT_PROMPT,\n        )\n        self.system_prompt_llm = FUSED_SYSTEM_PROMPT_WITH_SOP(\n            guidance=guidance_llm,\n            sop=GENERATE_TASKS_PROMPT,\n        )\n        self.system_prompt_rank = FUSED_SYSTEM_PROMPT_WITH_SOP(\n            guidance_rank,\n            FILTER_TASKS_SOP_PROMPT,\n        )\n        # 1st LLM to visualize the object in the scene using GPT4V\n        self.vllm = GPT4VisionAPI(\n            system_prompt=self.system_prompt_vllm,\n            openai_api_key=openai_api_key,\n            max_tokens=max_tokens,\n        )\n        # 2nd LLM to generate tasks for the robot to perform based on the objects in the scene\n        self.tasks_generator = OpenAIChat(\n            openai_api_key=openai_api_key,\n            max_tokens=max_tokens,\n            prefix_messages=[FUSED_SYSTEM_PROMPT_WITH_SOP()],"
        },
        {
            "comment": "This code snippet contains a Python class with a method \"run\" that executes an AutoRT system. It involves three language models (LLMs): 1st LLM for visualization, 2nd LLM for generating tasks, and a 3rd LLM to rank these generated tasks based on complexity and human assistance need. The \"vllm\" variable stores the result of running the 1st LLM on input text and image. The \"tasks\" variable holds the tasks generated by the 2nd LLM.",
            "location": "\"/media/root/Prima/works/AutoRT/docs/src/autort/main.py\":107-135",
            "content": "        )\n        # 3rd LLM to rank the tasks generated by the 2nd LLM based on complexity and necessity of human assistance\n        self.task_filter = OpenAIChat(\n            openai_api_key=openai_api_key,\n            max_tokens=max_tokens,\n            prefix_messages=[self.system_prompt_rank],\n        )\n    def run(self, text: str, img: str, *args, **kwargs):\n        \"\"\"\n        Runs the AutoRT system to execute tasks based on the given text and image.\n        Args:\n            text (str): The text input for the system.\n            img (str): The image input for the system.\n            *args: Additional positional arguments.\n            **kwargs: Additional keyword arguments.\n        Returns:\n            Any: The result of executing the robot model on the ranked tasks.\n        \"\"\"\n        # 1st LLM to visualize the object in the scene using GPT4V\n        vllm = self.vllm.run(text, img)\n        # 2nd LLM to generate tasks for the robot to perform based on the objects in the scene\n        tasks = self.tasks_generator(vllm)"
        },
        {
            "comment": "The code snippet initializes a 3rd LLM to rank tasks generated by the 2nd LLM based on complexity and human assistance necessity. It then returns the robot model's execution of these ranked tasks using images as input.",
            "location": "\"/media/root/Prima/works/AutoRT/docs/src/autort/main.py\":137-141",
            "content": "        # 3rd LLM to rank the tasks generated by the 2nd LLM based on complexity and necessity of human assistance\n        ranks = self.task_filter(tasks)\n        # Robot model to execute the tasks\n        return self.robot_model(ranks, img)"
        }
    ]
}