import os
from swarms import GPT4VisionAPI, OpenAIChat
from autort.prompts import (
    VISUALIZE_OBJECT_PROMPT,
    GENERATE_TASKS_PROMPT,
    FUSED_SYSTEM_PROMPT_WITH_SOP,
    FILTER_TASKS_SOP_PROMPT,
)
from dotenv import load_dotenv
from typing import Callable

load_dotenv()

openai_api_key = os.getenv("OPENAI_API_KEY")
gemini_api_key = os.getenv("GENIMI_API_KEY")

max_tokens = 1000

guidance_vllm = "Effectively visualize the object in the scene."
guidance_llm = "Generate tasks for the robot to perform."
guidance_rank = "Rank the tasks generated based on complexity and necessity of human assistance."


class AutoRT:
    """
    AutoRT class represents an automated robot task system.

    Args:
        openai_api_key (str): The API key for OpenAI.
        max_tokens (int, optional): The maximum number of tokens to use for text generation. Defaults to 1000.
        guidance_vllm (str, optional): The guidance for the first language model. Defaults to guidance_vllm.
        guidance_llm (str, optional): The guidance for the second language model. Defaults to guidance_llm.
        guidance_rank (str, optional): The guidance for the third language model. Defaults to guidance_rank.
        robot_model (Callable, optional): The robot model to execute the tasks. Defaults to None.

    Attributes:
        openai_api_key (str): The API key for OpenAI.
        max_tokens (int): The maximum number of tokens to use for text generation.
        guidance_vllm (str): The guidance for the first language model.
        guidance_llm (str): The guidance for the second language model.
        guidance_rank (str): The guidance for the third language model.
        robot_model (Callable): The robot model to execute the tasks.
        system_prompt_vllm (FUSED_SYSTEM_PROMPT_WITH_SOP): The system prompt for the first language model.
        system_prompt_llm (FUSED_SYSTEM_PROMPT_WITH_SOP): The system prompt for the second language model.
        system_prompt_rank (FUSED_SYSTEM_PROMPT_WITH_SOP): The system prompt for the third language model.
        vllm (GPT4VisionAPI): The first language model to visualize the object in the scene.
        llm (OpenAIChat): The second language model to generate tasks for the robot.
        filter_llm (OpenAIChat): The third language model to rank the tasks generated by the second language model.

    Methods:
        run(text: str, img: str, *args, **kwargs) -> Any:
            Runs the AutoRT system to execute tasks based on the given text and image.
            
    Examples:
        >>> from autort import AutoRT
        >>> autort = AutoRT(openai_api_key, max_tokens=1000)
        >>> autort.run("There is a bottle on the table.", "https://i.imgur.com/2qY9f8U.png")

    """

    def __init__(
        self,
        openai_api_key: str,
        max_tokens: int = 1000,
        guidance_vllm: str = guidance_vllm,
        guidance_llm: str = guidance_llm,
        guidance_rank: str = guidance_rank,
        robot_model: Callable = None,
        *args,
        **kwargs
    ):
        self.openai_api_key = openai_api_key
        self.max_tokens = max_tokens
        self.guidance_vllm = guidance_vllm
        self.guidance_llm = guidance_llm
        self.guidance_rank = guidance_rank
        self.robot_model = robot_model
        

        self.system_prompt_vllm = FUSED_SYSTEM_PROMPT_WITH_SOP(
            guidance=guidance_vllm,
            sop=VISUALIZE_OBJECT_PROMPT,
        )

        self.system_prompt_llm = FUSED_SYSTEM_PROMPT_WITH_SOP(
            guidance=guidance_llm,
            sop=GENERATE_TASKS_PROMPT,
        )

        self.system_prompt_rank = FUSED_SYSTEM_PROMPT_WITH_SOP(
            guidance_rank,
            FILTER_TASKS_SOP_PROMPT,
        )

        # 1st LLM to visualize the object in the scene using GPT4V
        self.vllm = GPT4VisionAPI(
            system_prompt=self.system_prompt_vllm,
            openai_api_key=openai_api_key,
            max_tokens=max_tokens,
        )


        # 2nd LLM to generate tasks for the robot to perform based on the objects in the scene
        self.tasks_generator = OpenAIChat(
            openai_api_key=openai_api_key,
            max_tokens=max_tokens,
            prefix_messages=[FUSED_SYSTEM_PROMPT_WITH_SOP()],
        )


        # 3rd LLM to rank the tasks generated by the 2nd LLM based on complexity and necessity of human assistance
        self.task_filter = OpenAIChat(
            openai_api_key=openai_api_key,
            max_tokens=max_tokens,
            prefix_messages=[self.system_prompt_rank],
        )

    def run(self, text: str, img: str, *args, **kwargs):
        """
        Runs the AutoRT system to execute tasks based on the given text and image.

        Args:
            text (str): The text input for the system.
            img (str): The image input for the system.
            *args: Additional positional arguments.
            **kwargs: Additional keyword arguments.

        Returns:
            Any: The result of executing the robot model on the ranked tasks.

        """
        # 1st LLM to visualize the object in the scene using GPT4V
        vllm = self.vllm.run(text, img)

        # 2nd LLM to generate tasks for the robot to perform based on the objects in the scene
        tasks = self.tasks_generator(vllm)

        # 3rd LLM to rank the tasks generated by the 2nd LLM based on complexity and necessity of human assistance
        ranks = self.task_filter(tasks)
        
        # Robot model to execute the tasks
        return self.robot_model(ranks, img)